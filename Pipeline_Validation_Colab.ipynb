{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Validation & Dashboard - Google Colab Notebook\n",
    "\n",
    "This notebook provides a comprehensive validation and showcase of the data pipeline functionality.\n",
    "\n",
    "**Features:**\n",
    "- AWS S3 data loading\n",
    "- Data transformation pipeline\n",
    "- Metrics calculation and visualization\n",
    "- Interactive dashboard\n",
    "- Complete end-to-end workflow\n",
    "\n",
    "**Author:** mrohitth  \n",
    "**Date:** 2026-01-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's clone the repository and install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/mrohitth/test_project1.git\n",
    "%cd test_project1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Additional packages for Colab\n",
    "!pip install -q ipywidgets\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS S3 Configuration\n",
    "\n",
    "Set up your AWS credentials to access S3 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Option 1: Manual input (recommended for security)\n",
    "print(\"Please enter your AWS credentials:\")\n",
    "AWS_ACCESS_KEY_ID = getpass.getpass('AWS Access Key ID: ')\n",
    "AWS_SECRET_ACCESS_KEY = getpass.getpass('AWS Secret Access Key: ')\n",
    "AWS_REGION = input('AWS Region (default: us-east-1): ') or 'us-east-1'\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "\n",
    "print(\"\\nâœ… AWS credentials configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/test_project1/src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Import custom modules\n",
    "from s3_loader import S3DataLoader\n",
    "from load_data import load_data, validate_data, get_project_root\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User Input - Data Source Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input widgets\n",
    "print(\"ðŸ“Š Data Source Configuration\\n\")\n",
    "\n",
    "data_source_type = widgets.Dropdown(\n",
    "    options=['S3 Bucket', 'Local File', 'Generate Sample Data'],\n",
    "    value='Generate Sample Data',\n",
    "    description='Data Source:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "s3_bucket = widgets.Text(\n",
    "    value='my-data-bucket',\n",
    "    description='S3 Bucket:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "s3_key = widgets.Text(\n",
    "    value='data/input/data.csv',\n",
    "    description='S3 Key:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "local_file = widgets.Text(\n",
    "    value='data/raw/sample.csv',\n",
    "    description='Local File:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "sample_size = widgets.IntSlider(\n",
    "    value=5000,\n",
    "    min=100,\n",
    "    max=50000,\n",
    "    step=100,\n",
    "    description='Sample Size:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_source_type)\n",
    "display(s3_bucket)\n",
    "display(s3_key)\n",
    "display(local_file)\n",
    "display(sample_size)\n",
    "\n",
    "print(\"\\nâœ… Configuration inputs ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading\n",
    "\n",
    "Load data from the configured source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(n_records=5000):\n",
    "    \"\"\"\n",
    "    Generate sample taxi trip data for demonstration.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = [datetime.now() - timedelta(days=x) for x in range(30)]\n",
    "    \n",
    "    data = {\n",
    "        'trip_id': range(1, n_records + 1),\n",
    "        'pickup_datetime': np.random.choice(dates, n_records),\n",
    "        'dropoff_datetime': [d + timedelta(minutes=np.random.randint(5, 60)) \n",
    "                            for d in np.random.choice(dates, n_records)],\n",
    "        'vendor_id': np.random.choice([1, 2, 3], n_records),\n",
    "        'passenger_count': np.random.choice([1, 2, 3, 4, 5, 6], n_records),\n",
    "        'trip_distance': np.random.uniform(0.5, 30, n_records),\n",
    "        'fare_amount': np.random.uniform(2.5, 100, n_records),\n",
    "        'tip_amount': np.random.uniform(0, 50, n_records),\n",
    "        'total_amount': np.random.uniform(2.5, 150, n_records),\n",
    "        'payment_type': np.random.choice(['Credit Card', 'Cash', 'Mobile'], n_records),\n",
    "        'pickup_location': np.random.choice(['Manhattan', 'Brooklyn', 'Queens', 'Bronx'], n_records),\n",
    "        'dropoff_location': np.random.choice(['Manhattan', 'Brooklyn', 'Queens', 'Bronx'], n_records),\n",
    "        'speed_mph': np.random.uniform(5, 60, n_records),\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some missing values for realism\n",
    "    missing_indices = np.random.choice(df.index, size=int(n_records * 0.02), replace=False)\n",
    "    df.loc[missing_indices, 'tip_amount'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data based on user selection\n",
    "print(\"ðŸ“¥ Loading data...\\n\")\n",
    "\n",
    "if data_source_type.value == 'S3 Bucket':\n",
    "    print(f\"Loading from S3: s3://{s3_bucket.value}/{s3_key.value}\")\n",
    "    s3_loader = S3DataLoader()\n",
    "    df = s3_loader.load_csv_from_s3(s3_bucket.value, s3_key.value)\n",
    "    print(f\"âœ… Loaded {len(df)} records from S3\")\n",
    "    \n",
    "elif data_source_type.value == 'Local File':\n",
    "    print(f\"Loading from local file: {local_file.value}\")\n",
    "    df = load_data(local_file.value)\n",
    "    print(f\"âœ… Loaded {len(df)} records from local file\")\n",
    "    \n",
    "else:  # Generate Sample Data\n",
    "    print(f\"Generating {sample_size.value} sample records...\")\n",
    "    df = generate_sample_data(sample_size.value)\n",
    "    print(f\"âœ… Generated {len(df)} sample records\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Transformation Pipeline\n",
    "\n",
    "Apply data cleaning and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Starting data transformation pipeline...\\n\")\n",
    "\n",
    "# Store original count\n",
    "original_count = len(df)\n",
    "print(f\"Original record count: {original_count:,}\")\n",
    "\n",
    "# 1. Remove duplicates\n",
    "df_clean = df.drop_duplicates(subset=['trip_id'])\n",
    "duplicates_removed = original_count - len(df_clean)\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n",
    "\n",
    "# 2. Handle missing values\n",
    "before_missing = df_clean.isnull().sum().sum()\n",
    "df_clean['tip_amount'].fillna(df_clean['tip_amount'].median(), inplace=True)\n",
    "df_clean.dropna(subset=['pickup_datetime', 'vendor_id', 'passenger_count'], inplace=True)\n",
    "after_missing = df_clean.isnull().sum().sum()\n",
    "print(f\"Missing values handled: {before_missing - after_missing}\")\n",
    "\n",
    "# 3. Validate ranges\n",
    "before_validation = len(df_clean)\n",
    "df_clean = df_clean[\n",
    "    (df_clean['passenger_count'] > 0) &\n",
    "    (df_clean['passenger_count'] <= 8) &\n",
    "    (df_clean['trip_distance'] >= 0) &\n",
    "    (df_clean['fare_amount'] >= 0) &\n",
    "    (df_clean['total_amount'] >= 0)\n",
    "]\n",
    "invalid_records = before_validation - len(df_clean)\n",
    "print(f\"Invalid records removed: {invalid_records}\")\n",
    "\n",
    "# 4. Convert data types\n",
    "df_clean['pickup_datetime'] = pd.to_datetime(df_clean['pickup_datetime'])\n",
    "df_clean['dropoff_datetime'] = pd.to_datetime(df_clean['dropoff_datetime'])\n",
    "df_clean['passenger_count'] = df_clean['passenger_count'].astype(int)\n",
    "\n",
    "# 5. Add derived features\n",
    "df_clean['trip_duration_minutes'] = (\n",
    "    df_clean['dropoff_datetime'] - df_clean['pickup_datetime']\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "df_clean['hour_of_day'] = df_clean['pickup_datetime'].dt.hour\n",
    "df_clean['day_of_week'] = df_clean['pickup_datetime'].dt.dayofweek\n",
    "df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6])\n",
    "\n",
    "final_count = len(df_clean)\n",
    "success_rate = (final_count / original_count) * 100\n",
    "\n",
    "print(f\"\\nâœ… Transformation complete!\")\n",
    "print(f\"Final record count: {final_count:,}\")\n",
    "print(f\"Records removed: {original_count - final_count:,}\")\n",
    "print(f\"Success rate: {success_rate:.2f}%\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nTransformed data sample:\")\n",
    "display(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metrics Calculation\n",
    "\n",
    "Calculate comprehensive metrics for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Calculating metrics...\\n\")\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Total Trips': len(df_clean),\n",
    "    'Unique Vendors': df_clean['vendor_id'].nunique(),\n",
    "    'Total Revenue': df_clean['total_amount'].sum(),\n",
    "    'Average Fare': df_clean['fare_amount'].mean(),\n",
    "    'Average Trip Distance': df_clean['trip_distance'].mean(),\n",
    "    'Average Passengers': df_clean['passenger_count'].mean(),\n",
    "    'Average Trip Duration (min)': df_clean['trip_duration_minutes'].mean(),\n",
    "    'Average Speed (mph)': df_clean['speed_mph'].mean(),\n",
    "    'Total Tips': df_clean['tip_amount'].sum(),\n",
    "    'Average Tip': df_clean['tip_amount'].mean(),\n",
    "    'Tip Rate (% of fare)': (df_clean['tip_amount'].sum() / df_clean['fare_amount'].sum()) * 100,\n",
    "    'Weekend Trips': df_clean['is_weekend'].sum(),\n",
    "    'Weekday Trips': (~df_clean['is_weekend']).sum(),\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE METRICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        if 'Revenue' in key or 'Fare' in key or 'Tip' in key and 'Rate' not in key:\n",
    "            print(f\"{key:.<40} ${value:,.2f}\")\n",
    "        else:\n",
    "            print(f\"{key:.<40} {value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"{key:.<40} {value:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store metrics for later use\n",
    "metrics_df = pd.DataFrame([metrics]).T\n",
    "metrics_df.columns = ['Value']\n",
    "\n",
    "print(\"\\nâœ… Metrics calculated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Visualizations\n",
    "\n",
    "Create comprehensive visualizations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure with subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "fig.suptitle('Data Pipeline Analysis Dashboard', fontsize=20, fontweight='bold', y=1.00)\n",
    "\n",
    "# 1. Trip Distribution by Vendor\n",
    "vendor_counts = df_clean['vendor_id'].value_counts().sort_index()\n",
    "axes[0, 0].bar(vendor_counts.index, vendor_counts.values, color='steelblue', alpha=0.8)\n",
    "axes[0, 0].set_title('Trip Distribution by Vendor', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Vendor ID')\n",
    "axes[0, 0].set_ylabel('Number of Trips')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Revenue by Vendor\n",
    "revenue_by_vendor = df_clean.groupby('vendor_id')['total_amount'].sum().sort_index()\n",
    "axes[0, 1].bar(revenue_by_vendor.index, revenue_by_vendor.values, color='green', alpha=0.8)\n",
    "axes[0, 1].set_title('Total Revenue by Vendor', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Vendor ID')\n",
    "axes[0, 1].set_ylabel('Revenue ($)')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Trip Distance Distribution\n",
    "axes[1, 0].hist(df_clean['trip_distance'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Trip Distance Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Distance (miles)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(df_clean['trip_distance'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {df_clean[\"trip_distance\"].mean():.2f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Fare Amount Distribution\n",
    "axes[1, 1].hist(df_clean['fare_amount'], bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Fare Amount Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Fare ($)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].axvline(df_clean['fare_amount'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: ${df_clean[\"fare_amount\"].mean():.2f}')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Trips by Hour of Day\n",
    "hourly_trips = df_clean['hour_of_day'].value_counts().sort_index()\n",
    "axes[2, 0].plot(hourly_trips.index, hourly_trips.values, marker='o', \n",
    "                linewidth=2, markersize=8, color='teal')\n",
    "axes[2, 0].fill_between(hourly_trips.index, hourly_trips.values, alpha=0.3, color='teal')\n",
    "axes[2, 0].set_title('Trips by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Hour')\n",
    "axes[2, 0].set_ylabel('Number of Trips')\n",
    "axes[2, 0].set_xticks(range(0, 24, 2))\n",
    "axes[2, 0].grid(alpha=0.3)\n",
    "\n",
    "# 6. Payment Type Distribution\n",
    "payment_dist = df_clean['payment_type'].value_counts()\n",
    "colors = plt.cm.Set3(range(len(payment_dist)))\n",
    "axes[2, 1].pie(payment_dist.values, labels=payment_dist.index, autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "axes[2, 1].set_title('Payment Type Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Analytics\n",
    "\n",
    "Additional insights and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Advanced Analytics Dashboard', fontsize=20, fontweight='bold', y=1.00)\n",
    "\n",
    "# 1. Average Fare by Passenger Count\n",
    "avg_fare_by_passengers = df_clean.groupby('passenger_count')['fare_amount'].mean()\n",
    "axes[0, 0].bar(avg_fare_by_passengers.index, avg_fare_by_passengers.values, \n",
    "               color='darkblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Average Fare by Passenger Count', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Passengers')\n",
    "axes[0, 0].set_ylabel('Average Fare ($)')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Weekend vs Weekday Comparison\n",
    "weekend_stats = df_clean.groupby('is_weekend')['total_amount'].agg(['count', 'sum', 'mean'])\n",
    "weekend_labels = ['Weekday', 'Weekend']\n",
    "x = np.arange(len(weekend_labels))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 1].bar(x - width/2, weekend_stats['count'], width, label='Trip Count', color='orange', alpha=0.7)\n",
    "axes[0, 1].set_ylabel('Trip Count', color='orange')\n",
    "axes[0, 1].tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "ax2 = axes[0, 1].twinx()\n",
    "ax2.bar(x + width/2, weekend_stats['mean'], width, label='Avg Revenue', color='blue', alpha=0.7)\n",
    "ax2.set_ylabel('Average Revenue ($)', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "axes[0, 1].set_title('Weekend vs Weekday Analysis', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(weekend_labels)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Top Routes\n",
    "top_routes = df_clean.groupby(['pickup_location', 'dropoff_location']).size().nlargest(10)\n",
    "route_labels = [f\"{p} â†’ {d}\" for p, d in top_routes.index]\n",
    "axes[1, 0].barh(range(len(top_routes)), top_routes.values, color='mediumseagreen', alpha=0.8)\n",
    "axes[1, 0].set_yticks(range(len(top_routes)))\n",
    "axes[1, 0].set_yticklabels(route_labels, fontsize=9)\n",
    "axes[1, 0].set_title('Top 10 Routes', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Trips')\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Speed Distribution by Vendor\n",
    "vendors = df_clean['vendor_id'].unique()\n",
    "for vendor in sorted(vendors):\n",
    "    vendor_data = df_clean[df_clean['vendor_id'] == vendor]['speed_mph']\n",
    "    axes[1, 1].hist(vendor_data, bins=30, alpha=0.5, label=f'Vendor {vendor}')\n",
    "\n",
    "axes[1, 1].set_title('Speed Distribution by Vendor', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Speed (mph)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Advanced analytics complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dashboard Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vendor performance summary table\n",
    "vendor_summary = df_clean.groupby('vendor_id').agg({\n",
    "    'trip_id': 'count',\n",
    "    'total_amount': ['sum', 'mean'],\n",
    "    'fare_amount': 'mean',\n",
    "    'trip_distance': 'mean',\n",
    "    'passenger_count': 'mean',\n",
    "    'speed_mph': 'mean',\n",
    "    'tip_amount': ['sum', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "vendor_summary.columns = [\n",
    "    'Total Trips', 'Total Revenue', 'Avg Revenue per Trip',\n",
    "    'Avg Fare', 'Avg Distance (mi)', 'Avg Passengers',\n",
    "    'Avg Speed (mph)', 'Total Tips', 'Avg Tip'\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"VENDOR PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 120)\n",
    "display(vendor_summary)\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results\n",
    "\n",
    "Save the processed data and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"ðŸ’¾ Exporting results...\\n\")\n",
    "\n",
    "# 1. Export transformed data\n",
    "csv_filename = f'transformed_data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "df_clean.to_csv(csv_filename, index=False)\n",
    "print(f\"âœ… Transformed data saved: {csv_filename}\")\n",
    "\n",
    "# 2. Export metrics\n",
    "metrics_filename = f'pipeline_metrics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(metrics_filename, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "print(f\"âœ… Metrics saved: {metrics_filename}\")\n",
    "\n",
    "# 3. Export vendor summary\n",
    "summary_filename = f'vendor_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "vendor_summary.to_csv(summary_filename)\n",
    "print(f\"âœ… Vendor summary saved: {summary_filename}\")\n",
    "\n",
    "# 4. Download files\n",
    "print(\"\\nðŸ“¥ Downloading files to your computer...\")\n",
    "files.download(csv_filename)\n",
    "files.download(metrics_filename)\n",
    "files.download(summary_filename)\n",
    "\n",
    "print(\"\\nâœ… All results exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optional: Upload Results to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload results back to S3 (optional)\n",
    "upload_to_s3 = input(\"Upload results to S3? (yes/no): \").lower() == 'yes'\n",
    "\n",
    "if upload_to_s3:\n",
    "    output_bucket = input(\"S3 Bucket for output: \")\n",
    "    output_prefix = input(\"S3 Prefix (e.g., 'output/'): \")\n",
    "    \n",
    "    s3_loader = S3DataLoader()\n",
    "    \n",
    "    # Upload transformed data\n",
    "    s3_loader.upload_dataframe(\n",
    "        df_clean, \n",
    "        output_bucket, \n",
    "        f\"{output_prefix}{csv_filename}\"\n",
    "    )\n",
    "    print(f\"âœ… Uploaded {csv_filename} to S3\")\n",
    "    \n",
    "    # Upload metrics\n",
    "    s3_loader.upload_file(\n",
    "        metrics_filename,\n",
    "        output_bucket,\n",
    "        f\"{output_prefix}{metrics_filename}\"\n",
    "    )\n",
    "    print(f\"âœ… Uploaded {metrics_filename} to S3\")\n",
    "    \n",
    "    # Upload vendor summary\n",
    "    s3_loader.upload_file(\n",
    "        summary_filename,\n",
    "        output_bucket,\n",
    "        f\"{output_prefix}{summary_filename}\"\n",
    "    )\n",
    "    print(f\"âœ… Uploaded {summary_filename} to S3\")\n",
    "    \n",
    "    print(\"\\nâœ… All results uploaded to S3!\")\n",
    "else:\n",
    "    print(\"Skipping S3 upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Interactive Dashboard (Streamlit in Colab)\n",
    "\n",
    "Launch the interactive dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Streamlit tunnel for Colab\n",
    "!pip install -q streamlit pyngrok\n",
    "\n",
    "# Save current data for dashboard\n",
    "df_clean.to_csv('data/processed/current_data.csv', index=False)\n",
    "\n",
    "print(\"\\nðŸš€ Starting dashboard...\")\n",
    "print(\"\\nNote: You can run the dashboard with:\")\n",
    "print(\"!streamlit run dashboard/app.py &\")\n",
    "print(\"\\nThen use ngrok to expose it publicly.\")\n",
    "print(\"\\nFor this demo, the dashboard files are prepared and ready to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Summary and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE EXECUTION COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nExecution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nData Source: {data_source_type.value}\")\n",
    "print(f\"Original Records: {original_count:,}\")\n",
    "print(f\"Final Records: {final_count:,}\")\n",
    "print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "print(f\"\\nTotal Revenue Processed: ${metrics['Total Revenue']:,.2f}\")\n",
    "print(f\"Average Transaction: ${metrics['Average Fare']:,.2f}\")\n",
    "print(f\"\\nFiles Generated:\")\n",
    "print(f\"  - {csv_filename}\")\n",
    "print(f\"  - {metrics_filename}\")\n",
    "print(f\"  - {summary_filename}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL PIPELINE TASKS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
